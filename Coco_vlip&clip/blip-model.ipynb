{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":6404747,"sourceType":"datasetVersion","datasetId":3693108},{"sourceId":8507491,"sourceType":"datasetVersion","datasetId":5078162},{"sourceId":8507499,"sourceType":"datasetVersion","datasetId":5078168}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2024-05-28T03:32:18.329233Z","iopub.execute_input":"2024-05-28T03:32:18.330040Z","iopub.status.idle":"2024-05-28T03:32:31.917309Z","shell.execute_reply.started":"2024-05-28T03:32:18.329999Z","shell.execute_reply":"2024-05-28T03:32:31.916381Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pycocotools\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom transformers import BlipProcessor, BlipModel\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\nno_label_count = 0\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# File paths and directories (Kaggle specific)\nIMAGE_DIR = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\nANNOTATION_FILE = '/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_train2017.json'\nOUTPUT_PATH = '/kaggle/working'\n\n# Load COCO annotations\ncoco = COCO(ANNOTATION_FILE)\nimage_ids = coco.getImgIds()\n\n# Load the pretrained BLIP Processor and BLIP model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\")\nmodel = BlipModel.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n\n# Move model to GPU if available\nmodel.to(device)\n\ndef extract_features(image_path):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\")\n        \n        # Move inputs to GPU if available\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model.get_image_features(**inputs)\n            features = outputs.squeeze().cpu().numpy()\n        \n        return features\n    \n    except Exception as e:\n        print(f\"Error extracting features for image {image_path}: {str(e)}\")\n        return np.array([])  # Return empty array on error\n\n# Feature extraction loop\nfeatures_list = []\nlabels_list = []\nimages_without_labels = []\n\n# Check if there are any previously saved features and labels\nif os.path.exists(os.path.join(OUTPUT_PATH, \"checkpoint.npy\")):\n    checkpoint = np.load(os.path.join(OUTPUT_PATH, \"checkpoint.npy\"), allow_pickle=True).item()\n    start_index = checkpoint['last_processed_index']\n    features_list = checkpoint['features_list']\n    labels_list = checkpoint['labels_list']\n    no_label_count = checkpoint['no_label_count']\n    images_without_labels = checkpoint['images_without_labels']\n    print(f\"Checkpoint found. Resuming from index {start_index}.\")\n\nelse:\n    start_index = 0\n\nfor i, img_id in enumerate(image_ids[start_index:], start=start_index):\n    img_info = coco.loadImgs(img_id)[0]\n    image_path = os.path.join(IMAGE_DIR, img_info['file_name'])\n    features = extract_features(image_path)\n    \n    if features.size == 0:\n        print(f\"Empty features for image: {image_path}\")\n        continue\n    \n    # Get annotations for the image\n    ann_ids = coco.getAnnIds(imgIds=img_id)\n    anns = coco.loadAnns(ann_ids)\n    label = [ann['category_id'] for ann in anns]  \n    \n    if not label:\n        no_label_count += 1\n        images_without_labels.append(image_path)\n        continue\n\n    features_list.append(features)\n    labels_list.append(label)\n    \n    if (i + 1) % 11800 == 0:\n        print(f\"{i + 1} images processed\")\n\n    # Save checkpoint every 500 images processed\n    if (i + 1) % 500 == 0:\n        checkpoint = {\n            'last_processed_index': i + 1,\n            'features_list': features_list,\n            'labels_list': labels_list,\n            'no_label_count': no_label_count,\n            'images_without_labels': images_without_labels\n        }\n        np.save(os.path.join(OUTPUT_PATH, \"checkpoint.npy\"), checkpoint)\n\n# Flatten the features_list if necessary\ntrain_features_array = np.vstack(features_list)\n\n# Convert labels_list to one-hot encoding\nunique_labels = sorted(set(label for labels in labels_list for label in labels))\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\n\ndef convert_to_one_hot(labels, label_map):\n    one_hot_labels = np.zeros(len(label_map))\n    for label in labels:\n        if label in label_map:  # Ensure label is in the label map\n            one_hot_labels[label_map[label]] = 1\n    return one_hot_labels\n\none_hot_labels_list = [convert_to_one_hot(labels, label_map) for labels in labels_list]\ntrain_labels_array = np.array(one_hot_labels_list)\n\n# Ensure train_features_array and train_labels_array are correctly shaped\nprint(\"Features array shape:\", train_features_array.shape)\nprint(\"Labels array shape:\", train_labels_array.shape)\n\n# Save final features and labels\nos.makedirs(OUTPUT_PATH, exist_ok=True)  # Ensure OUTPUT_PATH exists\nnp.save(os.path.join(OUTPUT_PATH, \"blip_train_features.npy\"), train_features_array)\nnp.save(os.path.join(OUTPUT_PATH, \"blip_train_labels.npy\"), train_labels_array)\n\n# Print images without labels\nprint(\"Total Images without labels: \", no_label_count)\nprint(\"Images without labels: \", images_without_labels)\n\n# Remove checkpoint file if processing completed successfully\nif os.path.exists(os.path.join(OUTPUT_PATH, \"checkpoint.npy\")):\n    os.remove(os.path.join(OUTPUT_PATH, \"checkpoint.npy\"))\n    print(\"Removed checkpoint file.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-28T07:26:34.151731Z","iopub.execute_input":"2024-05-28T07:26:34.152184Z","iopub.status.idle":"2024-05-28T07:27:09.915221Z","shell.execute_reply.started":"2024-05-28T07:26:34.152155Z","shell.execute_reply":"2024-05-28T07:27:09.913823Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Using device: cuda\nloading annotations into memory...\nDone (t=24.72s)\ncreating index...\nindex created!\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-large-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(features_file) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(labels_file):\n\u001b[1;32m     59\u001b[0m     features_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(features_file, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 60\u001b[0m     labels_list \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     61\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(features_list)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResuming from index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:436\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    434\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[1;32m    439\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n","\u001b[0;31mEOFError\u001b[0m: No data left in file"],"ename":"EOFError","evalue":"No data left in file","output_type":"error"}]},{"cell_type":"markdown","source":"**Test**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom transformers import BlipProcessor, BlipModel\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\nno_label_count: int = 0\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# File paths and directories (Kaggle specific)\nIMAGE_DIR = '/kaggle/input/coco-2017-dataset/coco2017/test2017'\nANNOTATION_FILE = '/kaggle/input/annotation-2017/image_info_test2017.json'\nOUTPUT_PATH = '/kaggle/working'\n\n# Load COCO annotations\ncoco = COCO(ANNOTATION_FILE)\nimage_ids = coco.getImgIds()\n\n# Load the pretrained BLIP Processor and BLIP model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\")\nmodel = BlipModel.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n\n# Move model to GPU if available\nmodel.to(device)\n\ndef extract_features(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    \n    # Move inputs to GPU if available\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.get_image_features(**inputs)\n        features = outputs.squeeze().cpu().numpy()\n    \n    return features\n\n# Feature extraction loop\nfeatures_list = []\nlabels_list = []\nimages_without_labels = []\n\nfor i, img_id in enumerate(image_ids):\n    img_info = coco.loadImgs(img_id)[0]\n    image_path = os.path.join(IMAGE_DIR, img_info['file_name'])\n    features = extract_features(image_path)\n    \n    # Ensure features are not empty\n    if features.size == 0:\n        print(f\"Empty features for image: {image_path}\")\n        continue\n    \n    # Get annotations for the image\n    ann_ids = coco.getAnnIds(imgIds=img_id)\n    anns = coco.loadAnns(ann_ids)\n    label = [ann['category_id'] for ann in anns]  \n    \n    if not label:\n        no_label_count += 1\n        images_without_labels.append(image_path)\n        continue\n\n    features_list.append(features)\n    labels_list.append(label)\n    \n    if i % 4000 == 0:\n        print(f\"{i} images processed\")\n\n# Flatten the features_list if necessary\ntest_features_array = np.vstack(features_list)\n\n# Convert labels_list to one-hot encoding\nunique_labels = sorted(set(label for labels in labels_list for label in labels))\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\n\ndef convert_to_one_hot(labels, label_map):\n    one_hot_labels = np.zeros(len(label_map))\n    for label in labels:\n        if label in label_map:  # Ensure label is in the label map\n            one_hot_labels[label_map[label]] = 1\n    return one_hot_labels\n\none_hot_labels_list = [convert_to_one_hot(labels, label_map) for labels in labels_list]\ntest_labels_array = np.array(one_hot_labels_list)\n\n# Ensure train_features_array and train_labels_array are correctly shaped\nprint(\"Features array shape:\", test_features_array.shape)\nprint(\"Labels array shape:\", test_labels_array.shape)\n\n# Save features and labels\nnp.save(os.path.join(OUTPUT_PATH, \"blip_test_features.npy\"), test_features_array)\nnp.save(os.path.join(OUTPUT_PATH, \"blip_test_labels.npy\"), test_labels_array)\n\n# Print images without labels\nprint(\"Total Images without labels: \", no_label_count)\nprint(\"Images without labels: \", images_without_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T17:26:14.997230Z","iopub.execute_input":"2024-05-27T17:26:14.998027Z","iopub.status.idle":"2024-05-27T17:26:15.061081Z","shell.execute_reply.started":"2024-05-27T17:26:14.997995Z","shell.execute_reply":"2024-05-27T17:26:15.059744Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlipProcessor, BlipForImageCaptioning\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomDataset\u001b[39;00m(Dataset):\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'BlipForImageCaptioning' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'BlipForImageCaptioning' from 'transformers' (/opt/conda/lib/python3.10/site-packages/transformers/__init__.py)","output_type":"error"}]},{"cell_type":"markdown","source":"**Val**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom transformers import BlipProcessor, BlipModel\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\nno_label_count = 0\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\n# File paths and directories (Kaggle specific)\nIMAGE_DIR = '/kaggle/input/coco-2017-dataset/coco2017/val2017'\nANNOTATION_FILE = '/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_val2017.json'\nOUTPUT_PATH = '/kaggle/working'\n\n# Load COCO annotations\ncoco = COCO(ANNOTATION_FILE)\nimage_ids = coco.getImgIds()\n\n# Load the pretrained BLIP Processor and BLIP model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\")\nmodel = BlipModel.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n\n# Move model to GPU if available\nmodel.to(device)\n\ndef extract_features(image_path):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, return_tensors=\"pt\")\n        \n        # Move inputs to GPU if available\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model.get_image_features(**inputs)\n            features = outputs.squeeze().cpu().numpy()\n        \n        return features\n    \n    except Exception as e:\n        print(f\"Error extracting features for image {image_path}: {str(e)}\")\n        return np.array([])  # Return empty array on error\n\n# Feature extraction loop\nfeatures_list = []\nlabels_list = []\nimages_without_labels = []\n\n# Check if there are any previously saved features and labels\nif os.path.exists(os.path.join(OUTPUT_PATH, \"checkpoint.npy\")):\n    checkpoint = np.load(os.path.join(OUTPUT_PATH, \"checkpoint.npy\"), allow_pickle=True).item()\n    start_index = checkpoint['last_processed_index']\n    features_list = checkpoint['features_list']\n    labels_list = checkpoint['labels_list']\n    no_label_count = checkpoint['no_label_count']\n    images_without_labels = checkpoint['images_without_labels']\n    print(f\"Checkpoint found. Resuming from index {start_index}.\")\n\nelse:\n    start_index = 0\n\nfor i, img_id in enumerate(image_ids[start_index:], start=start_index):\n    img_info = coco.loadImgs(img_id)[0]\n    image_path = os.path.join(IMAGE_DIR, img_info['file_name'])\n    features = extract_features(image_path)\n    \n    if features.size == 0:\n        print(f\"Empty features for image: {image_path}\")\n        continue\n    \n    # Get annotations for the image\n    ann_ids = coco.getAnnIds(imgIds=img_id)\n    anns = coco.loadAnns(ann_ids)\n    label = [ann['category_id'] for ann in anns]  \n    \n    if not label:\n        no_label_count += 1\n        images_without_labels.append(image_path)\n        continue\n\n    features_list.append(features)\n    labels_list.append(label)\n    \n    if (i + 1) % 500 == 0:\n        print(f\"{i + 1} images processed\")\n\n    # Save checkpoint every 10000 images processed\n    if (i + 1) % 500 == 0:\n        checkpoint = {\n            'last_processed_index': i + 1,\n            'features_list': features_list,\n            'labels_list': labels_list,\n            'no_label_count': no_label_count,\n            'images_without_labels': images_without_labels\n        }\n        np.save(os.path.join(OUTPUT_PATH, \"checkpoint.npy\"), checkpoint)\n\n# Flatten the features_list if necessary\nval_features_array = np.vstack(features_list)\n\n# Convert labels_list to one-hot encoding\nunique_labels = sorted(set(label for labels in labels_list for label in labels))\nlabel_map = {label: idx for idx, label in enumerate(unique_labels)}\n\ndef convert_to_one_hot(labels, label_map):\n    one_hot_labels = np.zeros(len(label_map))\n    for label in labels:\n        if label in label_map:  # Ensure label is in the label map\n            one_hot_labels[label_map[label]] = 1\n    return one_hot_labels\n\none_hot_labels_list = [convert_to_one_hot(labels, label_map) for labels in labels_list]\nval_labels_array = np.array(one_hot_labels_list)\n\n# Ensure train_features_array and train_labels_array are correctly shaped\nprint(\"Features array shape:\", val_features_array.shape)\nprint(\"Labels array shape:\", val_labels_array.shape)\n\n# Save final features and labels\nos.makedirs(OUTPUT_PATH, exist_ok=True)  # Ensure OUTPUT_PATH exists\nnp.save(os.path.join(OUTPUT_PATH, \"blip_val_features.npy\"), val_features_array)\nnp.save(os.path.join(OUTPUT_PATH, \"blip_val_labels.npy\"), val_labels_array)\n\n# Print images without labels\nprint(\"Total Images without labels: \", no_label_count)\nprint(\"Images without labels: \", images_without_labels)\n\n# Remove checkpoint file if processing completed successfully\nif os.path.exists(os.path.join(OUTPUT_PATH, \"checkpoint.npy\")):\n    os.remove(os.path.join(OUTPUT_PATH, \"checkpoint.npy\"))\n    print(\"Removed checkpoint file.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T19:31:24.158938Z","iopub.execute_input":"2024-05-27T19:31:24.159337Z","iopub.status.idle":"2024-05-27T19:39:32.095222Z","shell.execute_reply.started":"2024-05-27T19:31:24.159308Z","shell.execute_reply":"2024-05-27T19:39:32.094159Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Using device: cuda\nloading annotations into memory...\nDone (t=0.78s)\ncreating index...\nindex created!\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-itm-large-coco and are newly initialized: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.pooler.dense.bias', 'text_model.pooler.dense.weight', 'text_projection.weight', 'visual_projection.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"500 images processed\n1000 images processed\n1500 images processed\n2000 images processed\n2500 images processed\n3000 images processed\n3500 images processed\n4000 images processed\n4500 images processed\n5000 images processed\nFeatures array shape: (4952, 512)\nLabels array shape: (4952, 80)\nTotal Images without labels:  48\nImages without labels:  ['/kaggle/input/coco-2017-dataset/coco2017/val2017/000000226111.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000058636.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000458790.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000461275.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000268996.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000370999.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000481404.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000121153.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000310622.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000098497.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000228771.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000270386.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000240767.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000173183.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000560371.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000330554.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000550939.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000502910.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000382734.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000344611.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000127135.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000320706.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000064574.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000198915.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000101022.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000556498.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000308391.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000025593.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000536343.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000477118.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000528977.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000042888.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000041488.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000514540.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000374727.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000402096.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000200152.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000542073.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000476491.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000278006.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000261796.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000312549.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000260657.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000267946.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000049091.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000176701.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000404601.jpg', '/kaggle/input/coco-2017-dataset/coco2017/val2017/000000447789.jpg']\nRemoved checkpoint file.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**FCN with 1DCNN**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torchaudio\nimport torch\nimport tensorflow as tf\nimport sklearn.metrics\n\n\ntrain_features = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/train/wavlm_train_features.npy\")\ntrain_labels = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/train/wavlm_train_labels.npy\")\nval_features = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/val/wavlm_val_features.npy\")\nval_labels = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/val/wavlm_val_labels.npy\")\ntest_features = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/test/wavlm_test_features.npy\")\ntest_labels = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/test/wavlm_test_labels.npy\")\nprint(train_features.shape)\nprint(train_labels.shape)\nprint(val_features.shape)\nprint(val_labels.shape)\nprint(test_features.shape)\nprint(test_labels.shape)\ntrain_features = np.expand_dims(train_features, axis=2)\nval_features = np.expand_dims(val_features, axis=2)\ntest_features = np.expand_dims(test_features, axis=2)\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_fcn_model(input_shape, hidden_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n        tf.keras.layers.MaxPooling1D(pool_size=2),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n        tf.keras.layers.MaxPooling1D(pool_size=2),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(hidden_size, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(hidden_size // 2, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(hidden_size // 4, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(hidden_size // 8, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(1, activation = 'sigmoid')\n    ])\n    return model\n\n# Define model hyperparameters\ninput_shape = (768,1) #WavLM\nhidden_size = 1024\nlearning_rate = 0.0005\nnum_epochs = 20\nbatch_size = 32\nmodel = create_fcn_model(input_shape, hidden_size)\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define learning rate scheduling callback\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\n\n# Train the FCN model with learning rate scheduling\nmodel.fit(train_features, train_labels,\n          epochs=num_epochs,\n          batch_size=batch_size,\n          verbose=1,\n          validation_data=(val_features, val_labels),\n          callbacks=[lr_scheduler])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_scores = model.predict(test_features)  \n# Create a new NumPy array with the condition\n#pred_labels = tf.math.argmax(predicted_scores, axis=1)\nprint(predicted_scores.shape)\n\n\n# Define the compute_eer function\ndef compute_eer(label, pred, positive_label=1):\n    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n    fnr = 1 - tpr\n    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer = (eer_1 + eer_2) / 2\n    return eer\n\n# Calculate EER using the compute_eer function\neer = compute_eer(test_labels, predicted_scores)\n\nprint(\"Equal Error Rate (EER):\", eer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FCN Without 1D CNN**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torchaudio\nimport torch\nimport tensorflow as tf\nimport sklearn.metrics\n\ntrain_features = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/train/wavlm_train_features.npy\")\ntrain_labels = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/train/wavlm_train_labels.npy\")\nval_features = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/val/wavlm_val_features.npy\")\nval_labels = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/val/wavlm_val_labels.npy\")\ntest_features = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/test/wavlm_test_features.npy\")\ntest_labels = np.load(\"/kaggle/input/icasspembeddings/speech_icassp/speech/WavLM/Asvpoof_2019/test/wavlm_test_labels.npy\")\nprint(train_features.shape)\nprint(train_labels.shape)\nprint(val_features.shape)\nprint(val_labels.shape)\nprint(test_features.shape)\nprint(test_labels.shape)\ntrain_features = np.expand_dims(train_features, axis=2)\nval_features = np.expand_dims(val_features, axis=2)\ntest_features = np.expand_dims(test_features, axis=2)\nprint(train_features.shape)\nprint(val_features.shape)\nprint(test_features.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the FCN model architecture without 1DCNN\ndef create_fcn_model(input_shape, hidden_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Flatten(input_shape=input_shape),  # Flatten the input\n        tf.keras.layers.Dense(hidden_size, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(hidden_size // 2, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(hidden_size // 4, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(hidden_size // 8, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    return model\n\n# Define model hyperparameters\ninput_shape = (768,1) #WavLM\nhidden_size = 1024\nlearning_rate = 0.0005\nnum_epochs = 20\nbatch_size = 32\nmodel = create_fcn_model(input_shape, hidden_size)\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define learning rate scheduling callback\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\n\n# Train the FCN model with learning rate scheduling\nmodel.fit(train_features, train_labels,\n          epochs=num_epochs,\n          batch_size=batch_size,\n          verbose=1,\n          validation_data=(val_features, val_labels),\n          callbacks=[lr_scheduler])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_scores = model.predict(test_features)  \n# Create a new NumPy array with the condition\n#pred_labels = tf.math.argmax(predicted_scores, axis=1)\nprint(predicted_scores.shape)\n\n\n# Define the compute_eer function\ndef compute_eer(label, pred, positive_label=1):\n    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n    fnr = 1 - tpr\n    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer = (eer_1 + eer_2) / 2\n    return eer\n\n# Calculate EER using the compute_eer function\neer = compute_eer(test_labels, predicted_scores)\n\nprint(\"Equal Error Rate (EER):\", eer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}